{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642046dc-c567-458b-8357-65fe2ccf612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import glob,os,sys,argparse,shutil,gc,copy,warnings,random,logging,multiprocessing,time\n",
    "from datetime import timedelta\n",
    "from wsi import slide,filters,tiles,util\n",
    "import PIL,pyvips\n",
    "\n",
    "START_TIME = time.time()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a01d3-1b4c-4098-bd26-5cbb861723d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f425bb-8c99-4f0c-aff0-37a0542e71ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 18 909\n"
     ]
    }
   ],
   "source": [
    "STAGE = 'train'\n",
    "BASE_PAGE = 5\n",
    "\n",
    "DATA_DIR = os.environ['DIR_RAW_DATA']\n",
    "RAW_IMG_DIR = f'{DATA_DIR}/images'\n",
    "\n",
    "PAGE_IX_MULS ={i:2**(BASE_PAGE-i) for i in range(BASE_PAGE+1)}\n",
    "BASE_DIR = os.path.join(os.environ['DIR_PROCESSED_DATA'] ,'workspace')\n",
    "\n",
    "DIR_OUTPUT_TILES = f'{BASE_DIR}/{STAGE}/tiles'\n",
    "PAGES_TO_EXTRACT = [3]\n",
    "\n",
    "df = pd.read_csv(f'{DATA_DIR}/train.csv',sep=',')\n",
    "\n",
    "df['slide_id'] = df.filename.str.split('.').str[0]\n",
    "df['og_bbox'] = df.apply(lambda x: [x.x1,x.y1,x.x2,x.y2] ,axis=1)\n",
    "df['corr_bbox'] = df.apply(lambda x: [x.x1,x.y1,min(x.max_x,x.x2),min(x.max_y,x.y2)] ,axis=1)\n",
    "\n",
    "df['w'] = df.x2-df.x1\n",
    "df['h'] = df.y2-df.y1\n",
    "df['xpad'] = df.max_x - df.x2\n",
    "df['ypad'] = df.max_y - df.y2\n",
    "\n",
    "bads = ['7HxL729fl6_b_38839_78455_39604_79279', 'HpWI7vJms2_a_71182_57113_71902_57462', 'i9xm71KbYG_b_31467_113976_31986_114541', 'JvxiXClFKl_a_20316_9218_20451_9502', 'JvxiXClFKl_a_69751_37180_70029_37225', 'Lzx7XfUujk_a_13308_34726_13336_34737', 'Lzx7XfUujk_b_10468_84122_10792_84991', 'Lzx7XfUujk_b_15486_145827_15738_146587', 'Lzx7XfUujk_b_17181_145345_17593_145910', 'Lzx7XfUujk_b_53455_82217_54007_82559', 'Lzx7XfUujk_b_63207_142583_63569_143562', 'Lzx7XfUujk_b_64715_142167_65022_142754', 'Lzx7XfUujk_b_67782_116477_68140_117554', 'Lzx7XfUujk_b_8802_84908_9241_85324', 'rzsagNFXMn_a_15458_148084_15486_148204', 'TFrBjcO8nJ_b_63103_127967_63818_128712', 'WipCgQtJPE_b_25648_118162_26120_118743', 'yJxYpOCh6m_b_18928_24013_18941_24038']\n",
    "df['uniqid'] = df.slide_id + '_'+ df.og_bbox.apply(lambda x: '_'.join([str(i) for i in x]))\n",
    "df_bads = df[df.uniqid.isin(bads)].reset_index(drop=True)\n",
    "df = df[~df.uniqid.isin(bads)].reset_index(drop=True)\n",
    "print(f'dropped {len(df_bads)}',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf51a2c1-d750-41d8-8695-74a48908a2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = df.drop_duplicates(subset='filename').reset_index(drop=True)\n",
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209900d-6d88-4088-a06f-100a5d2362a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef93bb98-41f4-46d8-a963-d6329cb3eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SCORE_THRESH = 0.1; MAX_TILES_PER_PAGE=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3034463e-9285-4c2f-93c7-059cbe4d89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles_for_page(cur_page,base_tile_sz,slide_name,df_tiles):\n",
    "    patch_size = PATCH_SIZES_ACT[cur_page]\n",
    "    slide_img = pyvips.Image.new_from_file(f'{RAW_IMG_DIR}/{slide_name}.tif', page=cur_page)\n",
    "    RES_MUL = PAGE_IX_MULS[cur_page] #2**(base_page-cur_page)\n",
    "    \n",
    "    dir_output = f'{DIR_OUTPUT_TILES}/{base_tile_sz}/{cur_page}_{patch_size}/{slide_name}' #b,p,sz\n",
    "    dir_output_img = f'{dir_output}/img' #b,p,sz\n",
    "    os.makedirs(dir_output_img,exist_ok=True)\n",
    "    ds_tiles=[]\n",
    "    save_ctr=0\n",
    "    for idx, row in df_tiles.iterrows():\n",
    "        if save_ctr>=MAX_TILES_PER_PAGE: ##generated maximum tiles for page, exit\n",
    "            break\n",
    "        y = row['Row Start']\n",
    "        x = row['Col Start']\n",
    "\n",
    "        if (y<0 or x<0):\n",
    "            #print(f\"skipping: {slide_name}, bad coords x:{x} y:{y}\")  \n",
    "            continue\n",
    "        \n",
    "        \n",
    "        x1 = x*RES_MUL\n",
    "        y1 = y*RES_MUL\n",
    "        \n",
    "        region_width = region_height = patch_size#PATCH_SIZES_ACT[cur_page]\n",
    "        if x1 + region_width >slide_img.width:\n",
    "            _diff=slide_img.width-(x1+region_width)\n",
    "            region_width = slide_img.width - x1\n",
    "            #TODO better drop bad tiles logic\n",
    "            if idx>0:\n",
    "                # print(f'skipping {slide_name} since {x1} + {region_width} >{slide_img.width} by {_diff}, new width {region_width}')\n",
    "                continue\n",
    "        if y1 + region_height >slide_img.height:\n",
    "            _diff=slide_img.height-(y1+region_height)\n",
    "            region_height = slide_img.height - y1\n",
    "            if idx>0:\n",
    "                # print(f'skipping {slide_name} since {y1} + {region_height} >{slide_img.height} by {_diff} new height {region_height}')\n",
    "                continue\n",
    "                \n",
    "        try:\n",
    "            region = pyvips.Region.new(slide_img).fetch(x1, y1, region_width, region_height)\n",
    "            img = np.ndarray(\n",
    "                buffer=region,\n",
    "                dtype=np.uint8,\n",
    "                shape=(region_height, region_width, 3)) #rgb image\n",
    "            \n",
    "            img = PIL.Image.fromarray(img)\n",
    "            # img.save(f'{dir_output_img}/{row.tile_id}_{y1}_{x1}.jpeg', quality=90)\n",
    "            img.save(f'{dir_output_img}/{row.tile_id}_{y1}_{x1}.png', quality=90)\n",
    "            save_ctr+=1\n",
    "\n",
    "            row['w']=region_width\n",
    "            row['h']=region_height\n",
    "            row['swidth']=slide_img.width\n",
    "            row['sheight']=slide_img.height\n",
    "            \n",
    "            ds_tiles.append(row)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            #print(f'Failed for {slide_name}. x: {x}, y: {y} x1: {x1}, y1: {y1} reg_w: {region_width}, reg_h: {region_height} ')\n",
    "            #print(f'slide width: {slide_img.width} height: {slide_img.height}  cur_page: {cur_page}' )\n",
    "            print(ex)\n",
    "        \n",
    "    d = pd.DataFrame(ds_tiles)\n",
    "    d.to_csv(f'{dir_output}/tile_meta.csv',index=False)\n",
    "\n",
    "def generate_tiles_for_slide_list(slide_names,base_tile_sz,pages_to_extract):\n",
    "    for slide_name in slide_names:\n",
    "        # ##generate tiles\n",
    "        df = pd.read_csv(f'{slide.TILE_DATA_DIR}/{slide_name}-tile_data.csv',skiprows=14).sort_values(by='Score',ascending=False)\n",
    "        df['og_ntiles'] = len(df)\n",
    "        df = df[df.Score>=MIN_SCORE_THRESH]\n",
    "        \n",
    "        # for th in SCORE_THRESHS:\n",
    "        #     df1=df[df.Score>th]\n",
    "        #     if len(df1)>=MIN_TILES_PER_PAGE:\n",
    "        #         break\n",
    "        \n",
    "        # if th==0:\n",
    "        #     # df1 = df1.head(MIN_TILES_PER_PAGE)\n",
    "        #     print(f'Ignoring Score filter: {slide_name}')\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     #print('found data at th ',th)\n",
    "        #     df = df1\n",
    "            \n",
    "            \n",
    "        df = df.reset_index(drop=True)\n",
    "        df['tile_id'] = df.index\n",
    "        df['slide_id'] = slide_name\n",
    "        \n",
    "        #df['filename'] = df['slide_id'] + '.tif'\n",
    "        for page in pages_to_extract:\n",
    "            save_tiles_for_page(page,base_tile_sz,slide_name,df)\n",
    "        #gen_tiles(RAW_IMG_DIR,base_tile_sz,df,pages_to_extract)\n",
    "\n",
    "\n",
    "def multiprocess_generate_tiles(slides_list,base_tile_sz,pages_to_extract):\n",
    "    num_slides = len(slides_list)\n",
    "\n",
    "    num_processes = min(multiprocessing.cpu_count(),5)\n",
    "    pool = multiprocessing.Pool(num_processes)\n",
    "\n",
    "    if num_processes > num_slides:\n",
    "        num_processes = num_slides\n",
    "    \n",
    "    slides_per_process = num_slides / num_processes\n",
    "    tasks = []\n",
    "    for num_process in range(1, num_processes + 1):\n",
    "        start_index = (num_process - 1) * slides_per_process + 1\n",
    "        end_index = num_process * slides_per_process\n",
    "        start_index = int(start_index)\n",
    "        end_index = int(end_index)\n",
    "        sublist = slides_list[start_index - 1:end_index]\n",
    "        tasks.append((sublist,base_tile_sz,pages_to_extract))\n",
    "        #print(f\"Task # {num_process} Process slides {sublist}\")\n",
    "    \n",
    "  # start tasks\n",
    "    results = []\n",
    "    for t in tasks:\n",
    "        results.append(pool.apply_async(generate_tiles_for_slide_list, t))\n",
    "\n",
    "    for result in results:\n",
    "        _ = result.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34914748-b371-4f28-9e0b-76a2b0d51c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = meta.slide_id.tolist()#[:10]\n",
    "\n",
    "BASE_TILE_SIZES = [128,160,192,224,256]\n",
    "\n",
    "for BASE_TILE_SZ in BASE_TILE_SIZES:\n",
    "    tiles.TILE_SIZE_BASE = BASE_TILE_SZ\n",
    "    slide.TILE_DATA_DIR = os.path.join(slide.BASE_DIR, f\"tile_data/{BASE_TILE_SZ}\")\n",
    "    slide.TOP_TILES_DIR = os.path.join(slide.BASE_DIR, f\"top_tiles/{BASE_TILE_SZ}\")\n",
    "    PATCH_SIZES_ACT ={i:BASE_TILE_SZ*2**(BASE_PAGE-i) for i in range(BASE_PAGE)} #patch size to extract for each page\n",
    "    \n",
    "    \n",
    "    print(f'##### GENERATING TILE META {BASE_TILE_SZ} tile sizes: {PATCH_SIZES_ACT} #####')\n",
    "    tiles.multiprocess_filtered_images_to_tiles(image_list=NAMES, display=False, save_summary=False, save_data=True, save_top_tiles=False)\n",
    "    for PAGE in PAGES_TO_EXTRACT:\n",
    "        SIZE = PATCH_SIZES_ACT[PAGE]\n",
    "        print(f'##### GENERATING TILES {BASE_TILE_SZ}_{PAGE}_{SIZE} #####')\n",
    "        multiprocess_generate_tiles(NAMES,BASE_TILE_SZ,PAGES_TO_EXTRACT)\n",
    "    \n",
    "    elapsed = time.time() - START_TIME\n",
    "    print(f'##### DONE GENERATING TILES {BASE_TILE_SZ}_{PAGE}_{SIZE} TOTAL TIME: {timedelta(seconds=elapsed)} #####')\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
